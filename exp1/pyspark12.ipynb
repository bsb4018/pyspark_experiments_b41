{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyRFaPks98K4",
        "outputId": "a016dd95-236c-4349-c231-f1bd9e6a0d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=61a2343ad7935f50d9ac422dfac4dc36f1afc41a6304098acb235df03858344f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"drive/MyDrive/adv_analytics/all_blocks.csv\""
      ],
      "metadata": {
        "id": "zMIjYwaM-Cgj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "Xf-nOZGaAdxS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "NxerqHIXAsIE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('Adv_Analytics').getOrCreate()"
      ],
      "metadata": {
        "id": "l4yW4Od-AyRy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev = spark.read.csv(filepath)"
      ],
      "metadata": {
        "id": "biQcSN0j-CjI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbWK0qNP-Crq",
        "outputId": "237936b7-47c5-4388-8258-63c5ea5aec5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBWKTlrO-CuQ",
        "outputId": "df501c82-1856-4985-e4a5-ed811be7fa33"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "|  _c0|  _c1|              _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
            "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "|37291|53113|0.833333333333333|           ?|         1.0|           ?|      1|     1|     1|     1|      0|    True|\n",
            "|39086|47614|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    True|\n",
            "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing Schema Inference and Missing Values Set to Null -> the column names are set correctly and the ? strings have been replaced by null values"
      ],
      "metadata": {
        "id": "RYKXCzHNGUKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n",
        "option(\"inferSchema\", \"true\").csv(filepath)"
      ],
      "metadata": {
        "id": "e3yA_iQ4-CwZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbFv04QYBXcK",
        "outputId": "193e02dd-7d96-493a-e500-a06feaab686d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_1: integer (nullable = true)\n",
            " |-- id_2: integer (nullable = true)\n",
            " |-- cmp_fname_c1: double (nullable = true)\n",
            " |-- cmp_fname_c2: double (nullable = true)\n",
            " |-- cmp_lname_c1: double (nullable = true)\n",
            " |-- cmp_lname_c2: double (nullable = true)\n",
            " |-- cmp_sex: integer (nullable = true)\n",
            " |-- cmp_bd: integer (nullable = true)\n",
            " |-- cmp_bm: integer (nullable = true)\n",
            " |-- cmp_by: integer (nullable = true)\n",
            " |-- cmp_plz: integer (nullable = true)\n",
            " |-- is_match: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema Inference does two passes -> one pass to figure out the type of each column, and a second pass to do the actual parsing.\n",
        "# If you know the schema that you want to use for a file ahead of time, you can create an instance of the pyspark.sql.types.StructType \n",
        "# class and pass it to the Reader # API via the schema function. This can have a significant performance benefit when the dataset is \n",
        "# very large, since Spark will not need to perform an extra pass over the data to figure out the data type of each column."
      ],
      "metadata": {
        "id": "hY0S7r9eBXe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "bDjoHYBrBXha"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#schema = StructType([StructField(\"id_1\", IntegerType(), False),\n",
        "#StructField(\"id_2\", StringType(), False),\n",
        "#StructField(\"cmp_fname_c1\", DoubleType(), False)])"
      ],
      "metadata": {
        "id": "EkWzQ16VBXjy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parsed2 = spark.read.schema(schema).csv(filepath)"
      ],
      "metadata": {
        "id": "ou9Yd743BXmI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXHpuAd4BXq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames have a number of methods that enable us to read data from the cluster into the PySpark REPL on our client machine."
      ],
      "metadata": {
        "id": "JG6r_MO-BXte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed.first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXn5OjybBXvz",
        "outputId": "e765c376-fd12-43d5-8275-4f7213082405"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(id_1=37291, id_2=53113, cmp_fname_c1=0.833333333333333, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=0, is_match=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we know the dataset is small -> only then we can use the toPandas or collect method to return all the contents of a DataFrame to the client as an array. "
      ],
      "metadata": {
        "id": "ce2V8zxRBXyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing Data with the DataFrame API"
      ],
      "metadata": {
        "id": "qbuhYSs8BX07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting an idea of the number of records\n",
        "parsed.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37PJbMIDBX3U",
        "outputId": "4be461ef-1bb9-4b8d-c368-b0286938dc74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5749132"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whenever we ask another question -> do another computation, Spark will do these same operations, again and again, even if we have filtered the \n",
        "# Analyzing Data with the DataFrame API data down to a small number of records or are working with an aggregated version of the original dataset.\n",
        "# This isn’t an optimal use of our compute resources. After the data has been parsed once, we’d like to save the data in its parsed form on the \n",
        "# cluster so that we don’t have to reparse it every time\n",
        "parsed.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtBgCixIBX54",
        "outputId": "f873063c-cef2-467d-d437-3617d1cd6829"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to know is the relative fraction of records that were matches versus those that were nonmatches\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coBLJjZVBX8Q",
        "outputId": "5c4c56b4-070e-4613-cb17-9db5e926263c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|is_match|  count|\n",
            "+--------+-------+\n",
            "|   false|5728201|\n",
            "|    true|  20931|\n",
            "+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In addition to count, we can also compute more complex aggregations like sums, mins, maxes, means, and \n",
        "# standard deviation using the agg method of the DataFrame API in conjunction with the aggregation functions \n",
        "# defined in the pyspark.sql.functions collection\n",
        "\n",
        "from pyspark.sql.functions import avg, stddev\n",
        "parsed.agg(avg(\"cmp_sex\"), stddev(\"cmp_sex\")).show()"
      ],
      "metadata": {
        "id": "o06mPdRkBX-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdTGk6VfBYBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhXpapjqBYDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCVCBr20BYF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBkYDOuQBYIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NZQvxC6BYK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeIpnv3nBYNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dovOjvQJBYP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRfWNhbUBYSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcPWZsyvBYUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAdMU2XBBYXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iY_qDuVRBYZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1c_bWUZPBYcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3e1MLvgcBYej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tS5sVp4nBYhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nh5RMNlnBYjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zxaPPSYBYl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}